import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import timedelta
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# === CONFIGURA√á√ÉO ===
arquivo_json = "base.json"
n_periodos = 20 # N√∫mero de per√≠odos futuros a projetar
TAXA_DE_ANALISE = 0.55

# === 1. Carregar dados ===
with open(arquivo_json, "r") as f:
    data = json.load(f)
df = pd.DataFrame(data)

# === 2. Detectar colunas ===
# For√ßar identifica√ß√£o de coluna de tempo
if "__time" in df.columns:
    df["__time"] = pd.to_datetime(df["__time"], unit="ms")
    df.set_index("__time", inplace=True)

# Detectar colunas num√©ricas v√°lidas
value_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64'] and df[col].notna().sum() > 0.9 * len(df)]

if not value_cols:
    raise ValueError("Nenhuma coluna num√©rica v√°lida foi encontrada.")

# Usar todas as colunas num√©ricas
print(f"Sensores usados: {value_cols}")
df = df[value_cols].dropna()

# Calcular window_size dinamicamente com valida√ß√µes
total_periods = len(df)
max_window_size = 1000  # Definir um limite m√°ximo se necess√°rio
min_window_size = 24    # Definir um limite m√≠nimo (ex: 1 dia se dados hor√°rios)

# Calcular window_size como 80% dos dados, respeitando os limites
window_size = int(total_periods * TAXA_DE_ANALISE)
window_size = min(window_size, max_window_size)  # N√£o exceder o m√°ximo
window_size = max(window_size, min_window_size)  # N√£o ficar abaixo do m√≠nimo

print(f"Per√≠odos totais dispon√≠veis: {total_periods}")
print(f"Tamanho da janela de an√°lise: {window_size} per√≠odos")

# Detectar granularidade
granularidade = df.index.to_series().diff().median()
print(f"Granularidade detectada: {granularidade}")

# === 3. Normalizar ===
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# Modificar a fun√ß√£o create_dataset para trabalhar com m√∫ltiplas vari√°veis
def create_dataset(data, window_size=24):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)

X, y = create_dataset(scaled_data, window_size)
# Ajustar o reshape para considerar m√∫ltiplas features
X = X.reshape((X.shape[0], X.shape[1], len(value_cols)))

# Dividir treino/teste
split = int(len(X) * 0.8)
X_train, y_train = X[:split], y[:split]
X_test, y_test = X[split:], y[split:]

# === 4. Modelo LSTM ===
model = Sequential([
    LSTM(100, input_shape=(window_size, len(value_cols))),
    Dense(len(value_cols))
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=0)

# === 5. Previs√£o para todo o hist√≥rico ===
# Preparar todas as janelas poss√≠veis do hist√≥rico
todas_janelas = []
for i in range(len(scaled_data) - window_size):
    janela = scaled_data[i:i+window_size].reshape(1, window_size, len(value_cols))
    todas_janelas.append(janela)

# Fazer previs√µes para todo o hist√≥rico
previsoes_historico_scaled = []
for janela in todas_janelas:
    pred = model.predict(janela, verbose=0)[0]
    previsoes_historico_scaled.append(pred)

# Converter previs√µes para escala original
previsoes_historico = scaler.inverse_transform(np.array(previsoes_historico_scaled))

# Alinhar √≠ndices temporais para o hist√≥rico
indices_historico = df.index[window_size:]

# === 6. Proje√ß√£o futura ===
janela_atual = scaled_data[-window_size:].reshape(1, window_size, len(value_cols))
futuro_escalado = []

for _ in range(n_periodos):
    pred = model.predict(janela_atual, verbose=0)[0]
    futuro_escalado.append(pred)
    
    nova_janela = np.append(janela_atual[0, 1:, :], [pred], axis=0)
    janela_atual = nova_janela.reshape(1, window_size, len(value_cols))

futuro = scaler.inverse_transform(np.array(futuro_escalado))
datas_futuras = [df.index[-1] + granularidade * (i+1) for i in range(n_periodos)]

# === 7. Plot e salvar imagem ===
plt.figure(figsize=(20,10))

# Plotar dados para cada sensor
for i, col in enumerate(value_cols):
    # Plotar dados hist√≥ricos reais
    plt.plot(df.index, df[col].values, 
             label=f'Hist√≥rico Real ({col})', 
             alpha=0.7, 
             linewidth=2)
    
    # Plotar previs√µes hist√≥ricas
    #plt.plot(indices_historico, previsoes_historico[:, i], label=f'Hist√≥rico Previsto ({col})', linestyle='--', alpha=0.7)
    
    # Plotar proje√ß√£o futura
    plt.plot(datas_futuras, futuro[:, i], 
             label=f'Proje√ß√£o Futura ({col})', 
             linestyle=':', 
             marker='o', 
             alpha=0.7)

plt.title('An√°lise LSTM - Hist√≥rico Completo e Proje√ß√£o Futura')
plt.xlabel('Tempo')
plt.ylabel('Valores')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.savefig('previsao_lstm_completa.png', bbox_inches='tight', dpi=300)
plt.close()

# Calcular erro MSE para todo o hist√≥rico
mse_historico = {}
for i, col in enumerate(value_cols):
    mse = mean_squared_error(df[col].values[window_size:], previsoes_historico[:, i])
    mse_historico[col] = mse

print("‚úÖ An√°lise completa gerada com sucesso.")
print("üìÅ Gr√°fico salvo como previsao_lstm_completa.png")
print("\nüìä Erro MSE por sensor:")
for col, mse in mse_historico.items():
    print(f"- {col}: {mse:.6f}")
